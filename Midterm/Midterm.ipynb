{
 "metadata": {
  "name": "",
  "signature": "sha256:a440069b19a5341ec95f6a9e2d1caee383e3d8f7e204e8f5aca1263882004c98"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2 style=\"padding:0; margin:0; line-height:125%\">COP 6930: Web 2.0</h2>\n",
      "<h2 style=\"padding:0; margin:0; line-height:125%\">Midterm (Test 1)</h2>\n",
      "<h2 style=\"padding:0; margin:0; line-height:125%\">Debra Lewis</h2>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For my midterm, I will be gathering data from bitLy data recording the shortenings of government websites (from data downloaded from http://1usagov.measuredvoice.com/2013/ - I am using the data from May 17, 2013).  I will extract the most popular URLs from that data and then I will search twitter for tweets with those most popular URLs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first step is getting the pathways to the files containing the bitLy data.  All the files are stored in the folder <code>resources</code> which is in the <code>ipynb</code> directory. There are 25 files of data from May 17, 2013 (I changed their names to <code>data1.txt</code>, <code>data2.txt</code>, etc), so I used a <code>for</code> loop to open each of the files and add their contents to the records list.  Some of the files are empty, so they don't add any records to the list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = \"resources\"\n",
      "\n",
      "records = []\n",
      "\n",
      "for i in range(1,26):\n",
      "    file = open(path + '\\\\data' + str(i) + '.txt','r')\n",
      "    file.readline()\n",
      "    for line in file:\n",
      "        records.append(line)\n",
      "    file.close()\n",
      "    print \"Number of records after file\", i, \":\", len(records)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a regular expression pattern to extract only the url information, without extra characters.<br/>\n",
      "In order to use regular expressions, I'm importing the regular expressions module, which is <code>re</code>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "#search for 1.usa.gov in the hh field\n",
      "regex1 = re.compile('\\\"hh\\\": \\\"1\\.usa\\.gov\\\"')\n",
      "\n",
      "#regex string to grab the url\n",
      "regex2 = re.compile('\\\"u\\\": \\\"(.*?)\\\",')\n",
      "\n",
      "regex = re.compile( r\"\"\"\n",
      "                    \\\"hh\\\":\\ \\\"1\\.usa\\.gov\\\"\n",
      "                    .*?\n",
      "                    \\\"u\\\":\\ \\\"(.*?)\\\"\n",
      "                    \"\"\",re.VERBOSE)\n",
      "\n",
      "\n",
      "urls = []\n",
      "mnum = 0\n",
      "#the (str(m[0])).replace(\"\\/\",\"\\\\\") is to replace all \\/ combinations with a / to make it a proper url\n",
      "\n",
      "for record in records:\n",
      "    m = re.findall(regex,str(record))\n",
      "    if m:\n",
      "        urls.append((str(m[0])).replace(\"\\/\",\"/\"))\n",
      "   \n",
      "print urls[0]\n",
      "print \"Number of urls (with repetition): \" + str(len(urls))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a dictionary to hold each url and the number of times it appeared in the original list.  URLs will be used as an entry's key, and the number of times the URL appears will be used as the value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url_dict = {}\n",
      "\n",
      "for item in urls:\n",
      "    if item in url_dict:\n",
      "        url_dict[item] += 1\n",
      "    else:\n",
      "        url_dict[item] = 1\n",
      "        \n",
      "print url_dict.items()[0]\n",
      "print \"Number of unique URLs: \" + str(len(url_dict))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sort by frequency (in descending order), using <code>sorted</code> which returns a sorted list of tuples.  I'm using the <code>operator</code> module in order to be able to easily sort by the value of the dictionary entry.<br/>\n",
      "Afterwards, print the tuple for the most frequent url in order to check accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "sorted_unique_urls = sorted(url_dict.items(),key=operator.itemgetter(1),reverse=True)\n",
      "\n",
      "print sorted_unique_urls[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now grab the 50 most frequent urls and put them in a new list in order to search twitter for these urls.<br/>\n",
      "Afterwards, print the top entry from this list in order to ensure it contains the most frequent URL from previous steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frequent_urls = []\n",
      "\n",
      "for i in range (0,50):\n",
      "    frequent_urls.append(str(sorted_unique_urls[i][0]))\n",
      "    \n",
      "print frequent_urls[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, open a connection to twitter and authorize it.<br/>\n",
      "In order to interact with twitter, I'm using the python wrapper for the Twitter API by importing the <code>twitter</code> module.<br/>\n",
      "After connecting, print a message about the Twitter object (in order to indicate successful connection)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "\n",
      "CONSUMER_KEY = 'ydFa8cn6FIKKxJZsYsZ1Tsrn8'\n",
      "CONSUMER_SECRET = 'Bgs546mW1fUAdCTsSVZr2X2B36PXs9Ba4DG5E97NEMHZsBYakG'\n",
      "OAUTH_TOKEN = '2982700401-sHM9JfvR2FDpKWZ39kStsVvfHRo9WYAOkG3IdWL'\n",
      "OAUTH_TOKEN_SECRET = 'ytZVKWc5r9xq8dx2AmPJkb3gEC1rm8Znnp2YneNIh8Qkm'\n",
      "\n",
      "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n",
      "\n",
      "twitter_api = twitter.Twitter(auth=auth)\n",
      "\n",
      "print twitter_api"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next search twitter for the most frequent urls that were found. Matching tweets' information will be stored in the list <code>statuses</code>.<br/>\n",
      "Here I'm importing <code>unquote</code> from <code>urllib</code> in order to ensure the URLs stay readable, and I'm importing <code>json</code> in order to print an example in a readable format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib import unquote\n",
      "import json # pip install simplejson\n",
      "\n",
      "count = 100     #number of max results to return for each search\n",
      "\n",
      "query_count = 0\n",
      "max_queries = 180 #using this to avoid hitting the max query limit\n",
      "\n",
      "statuses = []\n",
      "\n",
      "for q in frequent_urls:\n",
      "    search_results = twitter_api.search.tweets(q=q,count=count)\n",
      "    query_count+=1\n",
      "    statuses += search_results['statuses']\n",
      "    \n",
      "    if query_count >= max_queries:\n",
      "        break\n",
      "    \n",
      "    for _ in range(5):\n",
      "        print \"Length of statuses\", len(statuses)\n",
      "        try:\n",
      "            next_results = search_results['search_metadata']['next_results']\n",
      "            query_count+=1\n",
      "        except KeyError, e: # No more results when next_results doesn't exist\n",
      "            break\n",
      "\n",
      "        kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
      "\n",
      "        search_results = twitter_api.search.tweets(**kwargs)\n",
      "        statuses += search_results['statuses']\n",
      "        if query_count >= max_queries:\n",
      "            break\n",
      "            \n",
      "        if _ == 5 and next_results:\n",
      "            print \"Limit reached for current url:\", q\n",
      "    \n",
      "if not statuses:\n",
      "    print \"No matches found\"\n",
      "else:\n",
      "    print \"Number of matches found:\", len(statuses)\n",
      "    print json.dumps(statuses[0],indent=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now extract relevant information from the matching texts (in <code>statuses</code>) in order to present it in a tabular format.<br/>\n",
      "The information we want is the user name of the person who tweeted, the url that was tweeted, the text of the tweet, and the location of the tweet.<br/>\n",
      "I'm importing <code>namedtuple</code> from the <code>collections</code> module in order to create my own tuple type so that it's easier to keep track of which field is being accessed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import namedtuple\n",
      "\n",
      "TweetInfo = namedtuple('TweetInfo','user, url, text, location')\n",
      "Point = namedtuple('Point','lat,lon')\n",
      "\n",
      "loc_regex = re.compile('\\'coordinates\\': \\[(-?\\d*?\\.?\\d*?), (-?\\d*?\\.?\\d*?)\\]')\n",
      "\n",
      "matching_tweets = []\n",
      "\n",
      "for status in statuses:\n",
      "    text = status['text']\n",
      "    user = status['user']['screen_name']\n",
      "    location = status['coordinates']\n",
      "    m = loc_regex.search(str(location))\n",
      "    if m:\n",
      "        #Note that the lat and lon are swapped below, because twitter's coordinates field stores it in the order [lon,lat]\n",
      "        #but the human-readable format is [lat,lon]\n",
      "        pt = Point(lat=float(m.group(2)),lon=float(m.group(1)))\n",
      "    else:\n",
      "        pt = Point(0,0)\n",
      "    for geturl in status['entities']['urls']:\n",
      "        url = geturl['expanded_url']\n",
      "    if text and user and geturl and location and pt.lat!=0 and pt.lon !=0:  #if all 4 exist, create a new match and add it to the list of matches\n",
      "        match = TweetInfo(user=user,url=url,text=text,location=pt)\n",
      "        matching_tweets.append(match)\n",
      "\n",
      "print matching_tweets[0]\n",
      "print \"number of matches with all info:\", len(matching_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Now, set up a pretty table to hold the information<Br/>\n",
      "This will only print the first 25 matching tweets, not all of the matches found.<br/>\n",
      "In order to print this in a more readable format, I will be importing the <code>PrettyTable</code> module from <code>prettytable</code><br/>\n",
      "Note: This will only look good if the browser window is long enough to accomodate all fields in the table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from prettytable import PrettyTable\n",
      "\n",
      "pt = PrettyTable(field_names=['Username','URL','Tweet text', 'Location (lat, lon)'])\n",
      "pt.max_width['Tweet text'] = 50\n",
      "pt.align = 'l'\n",
      "[ pt.add_row([row.user, row.url, row.text, \"(\" + '%.2f' % row.location.lat + \",\" + '%.2f' % row.location.lon + \")\"]) for row in matching_tweets[:50] ]\n",
      "print pt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Turning that information into a map</b><br/>\n",
      "Next we want to plot this coordinates on a map.<br/>\n",
      "Create a new maps object <code>mymap</code> using the pygmaps module.  Center it at 41.850033,-87.6500523 (over Chicago) and set it to a default zoom of 3 to give a decent window.</br/>\n",
      "Also create an array to hold the hex values for several colors (so that the different markers are distinguishible).<br/>\n",
      "Here, I'm importing <code>pygmaps</code>, a python wrapper for a google maps API in order to create the map and markers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pygmaps #pip install pygmaps\n",
      "mymap = pygmaps.maps(41.850033, -87.6500523,3)\n",
      "\n",
      "colors = ['#008000',\n",
      "          '#FFFF00',\n",
      "          '#FF0000',\n",
      "          '#0000FF']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add the coordinates of each point to the map.<br/>\n",
      "I'm randomizing the color for each point so that they are easier to distinguish if there's a large cluster of points close together.<br/>\n",
      "In order to have that randomization, I'm importing <code>random</code>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "\n",
      "for tweet in matching_tweets:\n",
      "    pt = tweet.location\n",
      "    mymap.addpoint(pt.lat,pt.lon,random.choice(colors))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, create the map (resulting html map is stored in the resources directory)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mymap.draw('resources\\mymap.html')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, load the newly created map into a frame in this notebook.<br/>\n",
      "I'm importing <code>HTML</code> from <code>IPython.display</code> in order to use HTML within this notebook to display the map."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('<iframe src=\"resources//mymap.html\" height=500 width=750></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}